{
    "docs": [
        {
            "location": "/", 
            "text": "mathtoolbox\n\n\nA library of mathematical tools (regression, interpolation, dimensionality reduction, clustering, etc.) written in C++11. Eigen \nhttp://eigen.tuxfamily.org/\n is used for the interface and internal vector/matrix representation.\n\n\n\n\nAlgorithms\n\n\nScattered Data Interpolation and Function Approximation\n\n\n\n\nrbf-interpolation\n: Radial basis function (RBF) network\n\n\ngaussian-process-regression\n: Gaussian process regression (GPR)\n\n\n\n\nDimensionality Reduction and Low-Dimensional Embedding\n\n\n\n\nclassical-mds\n: Classical multi-dimensional scaling (MDS)\n\n\n\n\nDependencies\n\n\n\n\nEigen \nhttp://eigen.tuxfamily.org/\n\n\nNLopt \nhttps://nlopt.readthedocs.io/\n (included as gitsubmodule)\n\n\nnlopt-util \nhttps://github.com/yuki-koyama/nlopt-util\n (included as gitsubmodule)\n\n\n\n\nBuild and Installation\n\n\nmathtoolbox uses CMake \nhttps://cmake.org/\n for building source codes. This library can be built, for example, by\n\n\ngit clone https://github.com/yuki-koyama/mathtoolbox.git --recursive\ncd mathtoolbox\nmkdir build\ncd build\ncmake ../\nmake\n\n\n\n\nand optionally it can be installed to the system by\n\n\nmake install\n\n\n\n\nWhen the CMake parameter \nMATHTOOLBOX_BUILD_EXAMPLES\n is set \nON\n, the example applications are also built. (The default setting is \nOFF\n.) This is done by, for example,\n\n\ncmake ../ -DMATHTOOLBOX_BUILD_EXAMPLES=ON\nmake\n\n\n\n\nInstalling Eigen\n\n\nIf you are using macOS, Eigen can be easily installed by\n\n\nbrew install eigen\n\n\n\n\nProjects Using mathtoolbox\n\n\n\n\nSelPh \nhttps://github.com/yuki-koyama/selph\n (for \nclassical-mds\n)\n\n\n\n\nLicensing\n\n\nThe MIT License.", 
            "title": "Introduction"
        }, 
        {
            "location": "/#mathtoolbox", 
            "text": "A library of mathematical tools (regression, interpolation, dimensionality reduction, clustering, etc.) written in C++11. Eigen  http://eigen.tuxfamily.org/  is used for the interface and internal vector/matrix representation.", 
            "title": "mathtoolbox"
        }, 
        {
            "location": "/#algorithms", 
            "text": "", 
            "title": "Algorithms"
        }, 
        {
            "location": "/#scattered-data-interpolation-and-function-approximation", 
            "text": "rbf-interpolation : Radial basis function (RBF) network  gaussian-process-regression : Gaussian process regression (GPR)", 
            "title": "Scattered Data Interpolation and Function Approximation"
        }, 
        {
            "location": "/#dimensionality-reduction-and-low-dimensional-embedding", 
            "text": "classical-mds : Classical multi-dimensional scaling (MDS)", 
            "title": "Dimensionality Reduction and Low-Dimensional Embedding"
        }, 
        {
            "location": "/#dependencies", 
            "text": "Eigen  http://eigen.tuxfamily.org/  NLopt  https://nlopt.readthedocs.io/  (included as gitsubmodule)  nlopt-util  https://github.com/yuki-koyama/nlopt-util  (included as gitsubmodule)", 
            "title": "Dependencies"
        }, 
        {
            "location": "/#build-and-installation", 
            "text": "mathtoolbox uses CMake  https://cmake.org/  for building source codes. This library can be built, for example, by  git clone https://github.com/yuki-koyama/mathtoolbox.git --recursive\ncd mathtoolbox\nmkdir build\ncd build\ncmake ../\nmake  and optionally it can be installed to the system by  make install  When the CMake parameter  MATHTOOLBOX_BUILD_EXAMPLES  is set  ON , the example applications are also built. (The default setting is  OFF .) This is done by, for example,  cmake ../ -DMATHTOOLBOX_BUILD_EXAMPLES=ON\nmake", 
            "title": "Build and Installation"
        }, 
        {
            "location": "/#installing-eigen", 
            "text": "If you are using macOS, Eigen can be easily installed by  brew install eigen", 
            "title": "Installing Eigen"
        }, 
        {
            "location": "/#projects-using-mathtoolbox", 
            "text": "SelPh  https://github.com/yuki-koyama/selph  (for  classical-mds )", 
            "title": "Projects Using mathtoolbox"
        }, 
        {
            "location": "/#licensing", 
            "text": "The MIT License.", 
            "title": "Licensing"
        }, 
        {
            "location": "/rbf-interpolation/", 
            "text": "rbf-interpolation\n\n\nRadial basis function (RBF) network for scattered data interpolation and function approximation.\n\n\nHeader\n\n\n#include \nmathtoolbox/rbf-interpolation.hpp\n\n\n\n\n\nMath\n\n\nOverview\n\n\nGiven input data:\n\n\n\n\n\n\\{ (\\mathbf{x}_i, y_i) \\}_{i = 1, \\ldots, n},\n\n\n\n\n\nthis technique calculates an interpolated value \n y \n for a specified point \n \\mathbf{x} \n by\n\n\n\n\n\ny = f(\\mathbf{x}) = \\sum_{i = 1}^{n} w_{i} \\phi( \\| \\mathbf{x} - \\mathbf{x}_{i} \\|),\n\n\n\n\n\nwhere \n \\phi(\\cdot) \n is a user-selected RBF and \n w_1, \\cdots, w_n \n are weights that are calculated in pre-computation.\n\n\n\n\nPre-Computation\n\n\nThe weight values need to be calculated in pre-computation. Let \n\n\n\n\n\n\\mathbf{w} = \\begin{bmatrix} w_1 & \\cdots & w_n \\end{bmatrix}^T\n\n\n\n\n\nand \n\n\n\n\n\n\\mathbf{\\Phi} = \n  \\begin{bmatrix} \n    \\phi_{1, 1} & \\cdots & \\phi_{1, n} \\\\\n    \\vdots      & \\ddots & \\vdots      \\\\\n    \\phi_{n, 1} & \\cdots & \\phi_{n, n}\n  \\end{bmatrix},\n\n\n\n\n\nwhere \n\n\n\n\n\n\\phi_{i, j} = \\phi(\\| \\mathbf{x}_i - \\mathbf{x}_j \\|).\n\n\n\n\n\nThe following linear system is solved for \n \\mathbf{w} \n:\n\n\n\n\n\n\\mathbf{\\Phi} \\mathbf{w} = \\mathbf{y}.\n\n\n\n\n\nLU decomposition can be used for solving this problem.\n\n\nPre-Computation with Regularization\n\n\nThe original formulation above is not robust (i.e., overfitting can occur) when the data points are dense and noisy.\n\n\n\n\nFor such scenarios, it is possible to add a \nregularization\n term into pre-computation. That is, the following minimization problem is solved:\n\n\n\n\n\n\\min_{\\mathbf{w}} \\left\\{ \\| \\mathbf{\\Phi} \\mathbf{w} - \\mathbf{y} \\|^2 + \\lambda \\| \\mathbf{w} \\|^2 \\right\\}.\n\n\n\n\n\nThe derivative of this objective function with respect to \n \\mathbf{w} \n is\n\n\n\n\n\n\\begin{eqnarray*}\n&& \\frac{\\partial}{\\partial \\mathbf{w}} \\left\\{ \\| \\mathbf{\\Phi} \\mathbf{w} - \\mathbf{y} \\|^2 + \\lambda \\| \\mathbf{w} \\|^2 \\right\\} \\\\\n&=& \\frac{\\partial}{\\partial \\mathbf{w}} \\| \\mathbf{\\Phi} \\mathbf{w} - \\mathbf{y} \\|^2 + \\lambda \\frac{\\partial}{\\partial \\mathbf{w}} \\| \\mathbf{w} \\|^2 \\\\\n&=& 2 \\mathbf{\\Phi}^T (\\mathbf{\\Phi} \\mathbf{w} - \\mathbf{y}) + 2 \\lambda \\mathbf{w} \\\\\n&=& 2 \\left\\{ (\\mathbf{\\Phi}^T \\mathbf{\\Phi} + \\lambda \\mathbf{I}) \\mathbf{w} - \\mathbf{\\Phi}^T \\mathbf{y} \\right\\}.\n\\end{eqnarray*}\n\n\n\n\n\nThus, the solution of the above minimization problem can be obtained by solving the below linear system:\n\n\n\n\n\n(\\mathbf{\\Phi}^T \\mathbf{\\Phi} + \\lambda \\mathbf{I}) \\mathbf{w} = \\mathbf{\\Phi}^T \\mathbf{y}.\n\n\n\n\n\n\n\nUsage\n\n\nFirst, instantiate the class \nRbfInterpolation\n. Via the constructor, an RBF can be specified from the following options:\n\n\n\n\nGaussian\n\n\nThinPlateSpline\n\n\nInverseQuadratic\n\n\nLinear\n\n\n\n\nBy default, \nThinPlateSpline\n (i.e., \n \\phi(x) = x^2 \\log(x) \n) is chosen.\n\n\nThen, set the target scattered data by the method\n\n\nvoid SetData(const Eigen::MatrixXd\n X, const Eigen::VectorXd\n y);\n\n\n\n\nwhere\n\n\n\n\n\n\\mathbf{X} = \\begin{bmatrix} \\mathbf{x}_{1} & \\cdots & \\mathbf{x}_{n} \\end{bmatrix} \\in \\mathbb{R}^{m \\times n}\n\n\n\n\n\nrepresents the data points and\n\n\n\n\n\n\\mathbf{y} = \\begin{bmatrix} y_1 & \\cdots & y_n \\end{bmatrix}^T\n\n\n\n\n\nrepresents their values.\n\n\nNext, calculate the weight values by the method:\n\n\nvoid ComputeWeights(bool use_regularization = false, double lambda = 0.001);\n\n\n\n\nWhen \nuse_regularization\n is set \ntrue\n, the weights are calculated in the manner of scattered data approximation, rather than scattered data interpolation. When the data is noisy, approximation is usually a better choice.\n\n\nOnce the above procedures are performed, the instance is ready to calculate interpolated values. This is performed by the method\n\n\ndouble GetValue(const Eigen::VectorXd\n x) const;\n\n\n\n\nUseful Resources\n\n\n\n\nKen Anjyo, J. P. Lewis, and Fr\u00e9d\u00e9ric Pighin. 2014. Scattered data interpolation for computer graphics. In ACM SIGGRAPH 2014 Courses (SIGGRAPH '14). ACM, New York, NY, USA, Article 27, 69 pages. DOI: \nhttps://doi.org/10.1145/2614028.2615425", 
            "title": "rbf-interpolation"
        }, 
        {
            "location": "/rbf-interpolation/#rbf-interpolation", 
            "text": "Radial basis function (RBF) network for scattered data interpolation and function approximation.", 
            "title": "rbf-interpolation"
        }, 
        {
            "location": "/rbf-interpolation/#header", 
            "text": "#include  mathtoolbox/rbf-interpolation.hpp", 
            "title": "Header"
        }, 
        {
            "location": "/rbf-interpolation/#math", 
            "text": "", 
            "title": "Math"
        }, 
        {
            "location": "/rbf-interpolation/#overview", 
            "text": "Given input data:   \n\\{ (\\mathbf{x}_i, y_i) \\}_{i = 1, \\ldots, n},   this technique calculates an interpolated value   y   for a specified point   \\mathbf{x}   by   \ny = f(\\mathbf{x}) = \\sum_{i = 1}^{n} w_{i} \\phi( \\| \\mathbf{x} - \\mathbf{x}_{i} \\|),   where   \\phi(\\cdot)   is a user-selected RBF and   w_1, \\cdots, w_n   are weights that are calculated in pre-computation.", 
            "title": "Overview"
        }, 
        {
            "location": "/rbf-interpolation/#pre-computation", 
            "text": "The weight values need to be calculated in pre-computation. Let    \n\\mathbf{w} = \\begin{bmatrix} w_1 & \\cdots & w_n \\end{bmatrix}^T   and    \n\\mathbf{\\Phi} = \n  \\begin{bmatrix} \n    \\phi_{1, 1} & \\cdots & \\phi_{1, n} \\\\\n    \\vdots      & \\ddots & \\vdots      \\\\\n    \\phi_{n, 1} & \\cdots & \\phi_{n, n}\n  \\end{bmatrix},   where    \n\\phi_{i, j} = \\phi(\\| \\mathbf{x}_i - \\mathbf{x}_j \\|).   The following linear system is solved for   \\mathbf{w}  :   \n\\mathbf{\\Phi} \\mathbf{w} = \\mathbf{y}.   LU decomposition can be used for solving this problem.", 
            "title": "Pre-Computation"
        }, 
        {
            "location": "/rbf-interpolation/#pre-computation-with-regularization", 
            "text": "The original formulation above is not robust (i.e., overfitting can occur) when the data points are dense and noisy.   For such scenarios, it is possible to add a  regularization  term into pre-computation. That is, the following minimization problem is solved:   \n\\min_{\\mathbf{w}} \\left\\{ \\| \\mathbf{\\Phi} \\mathbf{w} - \\mathbf{y} \\|^2 + \\lambda \\| \\mathbf{w} \\|^2 \\right\\}.   The derivative of this objective function with respect to   \\mathbf{w}   is   \n\\begin{eqnarray*}\n&& \\frac{\\partial}{\\partial \\mathbf{w}} \\left\\{ \\| \\mathbf{\\Phi} \\mathbf{w} - \\mathbf{y} \\|^2 + \\lambda \\| \\mathbf{w} \\|^2 \\right\\} \\\\\n&=& \\frac{\\partial}{\\partial \\mathbf{w}} \\| \\mathbf{\\Phi} \\mathbf{w} - \\mathbf{y} \\|^2 + \\lambda \\frac{\\partial}{\\partial \\mathbf{w}} \\| \\mathbf{w} \\|^2 \\\\\n&=& 2 \\mathbf{\\Phi}^T (\\mathbf{\\Phi} \\mathbf{w} - \\mathbf{y}) + 2 \\lambda \\mathbf{w} \\\\\n&=& 2 \\left\\{ (\\mathbf{\\Phi}^T \\mathbf{\\Phi} + \\lambda \\mathbf{I}) \\mathbf{w} - \\mathbf{\\Phi}^T \\mathbf{y} \\right\\}.\n\\end{eqnarray*}   Thus, the solution of the above minimization problem can be obtained by solving the below linear system:   \n(\\mathbf{\\Phi}^T \\mathbf{\\Phi} + \\lambda \\mathbf{I}) \\mathbf{w} = \\mathbf{\\Phi}^T \\mathbf{y}.", 
            "title": "Pre-Computation with Regularization"
        }, 
        {
            "location": "/rbf-interpolation/#usage", 
            "text": "First, instantiate the class  RbfInterpolation . Via the constructor, an RBF can be specified from the following options:   Gaussian  ThinPlateSpline  InverseQuadratic  Linear   By default,  ThinPlateSpline  (i.e.,   \\phi(x) = x^2 \\log(x)  ) is chosen.  Then, set the target scattered data by the method  void SetData(const Eigen::MatrixXd  X, const Eigen::VectorXd  y);  where   \n\\mathbf{X} = \\begin{bmatrix} \\mathbf{x}_{1} & \\cdots & \\mathbf{x}_{n} \\end{bmatrix} \\in \\mathbb{R}^{m \\times n}   represents the data points and   \n\\mathbf{y} = \\begin{bmatrix} y_1 & \\cdots & y_n \\end{bmatrix}^T   represents their values.  Next, calculate the weight values by the method:  void ComputeWeights(bool use_regularization = false, double lambda = 0.001);  When  use_regularization  is set  true , the weights are calculated in the manner of scattered data approximation, rather than scattered data interpolation. When the data is noisy, approximation is usually a better choice.  Once the above procedures are performed, the instance is ready to calculate interpolated values. This is performed by the method  double GetValue(const Eigen::VectorXd  x) const;", 
            "title": "Usage"
        }, 
        {
            "location": "/rbf-interpolation/#useful-resources", 
            "text": "Ken Anjyo, J. P. Lewis, and Fr\u00e9d\u00e9ric Pighin. 2014. Scattered data interpolation for computer graphics. In ACM SIGGRAPH 2014 Courses (SIGGRAPH '14). ACM, New York, NY, USA, Article 27, 69 pages. DOI:  https://doi.org/10.1145/2614028.2615425", 
            "title": "Useful Resources"
        }, 
        {
            "location": "/gaussian-process-regression/", 
            "text": "gaussian-process-regression\n\n\nGaussian process regression (GPR) for scattered data interpolation and function approximation.\n\n\n\n\nHeader\n\n\n#include \nmathtoolbox/gaussian-process-regression.hpp\n\n\n\n\n\nOverview\n\n\nInput\n\n\nThe input consists of a set of \n N \n scattered data points:\n\n\n\n\n\n\\{ (\\mathbf{x}_i, y_i) \\}_{i = 1, \\ldots, N},\n\n\n\n\n\nwhere \n \\mathbf{x}_i \\in \\mathbb{R}^D \n is the \n i \n-th data point location in a \n D \n-dimensional space and \n y_i \\in \\mathbb{R} \n is its associated value. This input data is also denoted as\n\n\n\n\n\n\\mathbf{X} = \\begin{bmatrix} \\mathbf{x}_{1} & \\cdots & \\mathbf{x}_{N} \\end{bmatrix} \\in \\mathbb{R}^{D \\times N}\n\n\n\n\n\nand\n\n\n\n\n\n\\mathbf{y} = \\begin{bmatrix} y_1 \\\\ \\vdots \\\\ y_N \\end{bmatrix} \\in \\mathbb{R}^{N}.\n\n\n\n\n\nOutput\n\n\nGiven the data and some \"Gaussian process\" assumptions, GPR can calculate the most likely value \n y \n and its variance \n \\text{Var}(y) \n for an arbitrary location \n \\mathbf{x} \n. \n\n\nThe variance roughly indicates how uncertain the estimation is. For example, when this value is large, the estimated value may not be very trustful (this often occurs in regions with less data points).\n\n\nNote that a 95%-confidence interval can be obtained by \n [ y - 1.96 \\sqrt{\\text{Var}(y)}, y + 1.96 \\sqrt{\\text{Var}(y)} ] \n.\n\n\nMath\n\n\nCoveriance Function\n\n\nThe automatic relevance determination (ARD) squared exponential kernel is used:\n\n\n\n\n\nk(\\mathbf{x}_p, \\mathbf{x}_q) = \\sigma_f^{2} \\exp \\left( - \\frac{1}{2} (\\mathbf{x}_p - \\mathbf{x}_q)^{T} \\text{diag}(\\boldsymbol{\\ell})^{-2} (\\mathbf{x}_p - \\mathbf{x}_q) \\right) + \\sigma_n^{2} \\delta_{pq},\n\n\n\n\n\nwhere \n \\sigma_f^{2} \n (the signal variance), \n \\sigma_n^{2} \n (the noise level), and \n \\boldsymbol{\\ell} \n (the characteristic length-scales) are hyperparameters.\n\n\nMean Function\n\n\nA constant-value function is used:\n\n\n\n\n\nm(\\mathbf{x}) = 0.\n\n\n\n\n\nSelecting Hyperparameters\n\n\nThere are two options for setting hyperparameters:\n\n\n\n\nSet manually\n\n\nDetermined by the maximum likelihood estimation\n\n\n\n\nMaximum Likelihood Estimation\n\n\nLet \n \\boldsymbol{\\theta} \n be a concatenation of hyperparameters; that is, \n\n\n\n\n\n\\boldsymbol{\\theta} = \\begin{bmatrix} \\sigma_{f}^{2} \\\\ \\sigma_{n}^{2} \\\\ \\boldsymbol{\\ell} \\end{bmatrix} \\in \\mathbb{R}^{D + 2}.\n\n\n\n\n\nIn this approach, these hyperparameters are determined by solving the following numerical optimization problem:\n\n\n\n\n\n\\boldsymbol{\\theta}^\\text{ML} = \\mathop{\\rm arg~max}\\limits_{\\boldsymbol{\\theta}} p(\\mathbf{y} \\mid \\mathbf{X}, \\boldsymbol{\\theta}).\n\n\n\n\n\nIn this implementation, this maximization problem is solved by the L-BFGS method (a gradient-based local optimization algorithm) from the NLopt library \nhttps://nlopt.readthedocs.io/\n. Initial solutions for this maximization need to be specified.\n\n\nUsage\n\n\nInstantiation and Data Specification\n\n\nA GPR object is instantiated with data specification in its constructor:\n\n\nGaussianProcessRegression(const Eigen::MatrixXd\n X, const Eigen::VectorXd\n y);\n\n\n\n\nHyperparameter Selection\n\n\nHyperparameters are set by either\n\n\nvoid SetHyperparameters(double sigma_squared_f,\n                        double sigma_squared_n,\n                        const Eigen::VectorXd\n length_scales);\n\n\n\n\nor \n\n\nvoid PerformMaximumLikelihood(double sigma_squared_f_initial,\n                              double sigma_squared_n_initial,\n                              const Eigen::VectorXd\n length_scales_initial);\n\n\n\n\nEstimation\n\n\nOnce a GPR object is instantiated and its hyperparameters are set, it is ready for estimation. For an unknown location \n \\mathbf{x} \n, the GPR object estimates the most likely value \n y \n by the following method:\n\n\ndouble EstimateY(const Eigen::VectorXd\n x) const;\n\n\n\n\nIt also estimates the variance \n \\text{Var}(y) \n by the following method:\n\n\ndouble EstimateVariance(const Eigen::VectorXd\n x) const;\n\n\n\n\nUseful Resources\n\n\n\n\nMark Ebden. 2015. Gaussian Processes: A Quick Introduction. \narXiv:1505.02965\n.\n\n\nCarl Edward Rasmussen and Christopher K. I. Williams. 2006. Gaussian Processes for Machine Learning. The MIT Press. Online version: \nhttp://www.gaussianprocess.org/gpml/", 
            "title": "gaussian-process-regression"
        }, 
        {
            "location": "/gaussian-process-regression/#gaussian-process-regression", 
            "text": "Gaussian process regression (GPR) for scattered data interpolation and function approximation.", 
            "title": "gaussian-process-regression"
        }, 
        {
            "location": "/gaussian-process-regression/#header", 
            "text": "#include  mathtoolbox/gaussian-process-regression.hpp", 
            "title": "Header"
        }, 
        {
            "location": "/gaussian-process-regression/#overview", 
            "text": "", 
            "title": "Overview"
        }, 
        {
            "location": "/gaussian-process-regression/#input", 
            "text": "The input consists of a set of   N   scattered data points:   \n\\{ (\\mathbf{x}_i, y_i) \\}_{i = 1, \\ldots, N},   where   \\mathbf{x}_i \\in \\mathbb{R}^D   is the   i  -th data point location in a   D  -dimensional space and   y_i \\in \\mathbb{R}   is its associated value. This input data is also denoted as   \n\\mathbf{X} = \\begin{bmatrix} \\mathbf{x}_{1} & \\cdots & \\mathbf{x}_{N} \\end{bmatrix} \\in \\mathbb{R}^{D \\times N}   and   \n\\mathbf{y} = \\begin{bmatrix} y_1 \\\\ \\vdots \\\\ y_N \\end{bmatrix} \\in \\mathbb{R}^{N}.", 
            "title": "Input"
        }, 
        {
            "location": "/gaussian-process-regression/#output", 
            "text": "Given the data and some \"Gaussian process\" assumptions, GPR can calculate the most likely value   y   and its variance   \\text{Var}(y)   for an arbitrary location   \\mathbf{x}  .   The variance roughly indicates how uncertain the estimation is. For example, when this value is large, the estimated value may not be very trustful (this often occurs in regions with less data points).  Note that a 95%-confidence interval can be obtained by   [ y - 1.96 \\sqrt{\\text{Var}(y)}, y + 1.96 \\sqrt{\\text{Var}(y)} ]  .", 
            "title": "Output"
        }, 
        {
            "location": "/gaussian-process-regression/#math", 
            "text": "", 
            "title": "Math"
        }, 
        {
            "location": "/gaussian-process-regression/#coveriance-function", 
            "text": "The automatic relevance determination (ARD) squared exponential kernel is used:   \nk(\\mathbf{x}_p, \\mathbf{x}_q) = \\sigma_f^{2} \\exp \\left( - \\frac{1}{2} (\\mathbf{x}_p - \\mathbf{x}_q)^{T} \\text{diag}(\\boldsymbol{\\ell})^{-2} (\\mathbf{x}_p - \\mathbf{x}_q) \\right) + \\sigma_n^{2} \\delta_{pq},   where   \\sigma_f^{2}   (the signal variance),   \\sigma_n^{2}   (the noise level), and   \\boldsymbol{\\ell}   (the characteristic length-scales) are hyperparameters.", 
            "title": "Coveriance Function"
        }, 
        {
            "location": "/gaussian-process-regression/#mean-function", 
            "text": "A constant-value function is used:   \nm(\\mathbf{x}) = 0.", 
            "title": "Mean Function"
        }, 
        {
            "location": "/gaussian-process-regression/#selecting-hyperparameters", 
            "text": "There are two options for setting hyperparameters:   Set manually  Determined by the maximum likelihood estimation", 
            "title": "Selecting Hyperparameters"
        }, 
        {
            "location": "/gaussian-process-regression/#maximum-likelihood-estimation", 
            "text": "Let   \\boldsymbol{\\theta}   be a concatenation of hyperparameters; that is,    \n\\boldsymbol{\\theta} = \\begin{bmatrix} \\sigma_{f}^{2} \\\\ \\sigma_{n}^{2} \\\\ \\boldsymbol{\\ell} \\end{bmatrix} \\in \\mathbb{R}^{D + 2}.   In this approach, these hyperparameters are determined by solving the following numerical optimization problem:   \n\\boldsymbol{\\theta}^\\text{ML} = \\mathop{\\rm arg~max}\\limits_{\\boldsymbol{\\theta}} p(\\mathbf{y} \\mid \\mathbf{X}, \\boldsymbol{\\theta}).   In this implementation, this maximization problem is solved by the L-BFGS method (a gradient-based local optimization algorithm) from the NLopt library  https://nlopt.readthedocs.io/ . Initial solutions for this maximization need to be specified.", 
            "title": "Maximum Likelihood Estimation"
        }, 
        {
            "location": "/gaussian-process-regression/#usage", 
            "text": "", 
            "title": "Usage"
        }, 
        {
            "location": "/gaussian-process-regression/#instantiation-and-data-specification", 
            "text": "A GPR object is instantiated with data specification in its constructor:  GaussianProcessRegression(const Eigen::MatrixXd  X, const Eigen::VectorXd  y);", 
            "title": "Instantiation and Data Specification"
        }, 
        {
            "location": "/gaussian-process-regression/#hyperparameter-selection", 
            "text": "Hyperparameters are set by either  void SetHyperparameters(double sigma_squared_f,\n                        double sigma_squared_n,\n                        const Eigen::VectorXd  length_scales);  or   void PerformMaximumLikelihood(double sigma_squared_f_initial,\n                              double sigma_squared_n_initial,\n                              const Eigen::VectorXd  length_scales_initial);", 
            "title": "Hyperparameter Selection"
        }, 
        {
            "location": "/gaussian-process-regression/#estimation", 
            "text": "Once a GPR object is instantiated and its hyperparameters are set, it is ready for estimation. For an unknown location   \\mathbf{x}  , the GPR object estimates the most likely value   y   by the following method:  double EstimateY(const Eigen::VectorXd  x) const;  It also estimates the variance   \\text{Var}(y)   by the following method:  double EstimateVariance(const Eigen::VectorXd  x) const;", 
            "title": "Estimation"
        }, 
        {
            "location": "/gaussian-process-regression/#useful-resources", 
            "text": "Mark Ebden. 2015. Gaussian Processes: A Quick Introduction.  arXiv:1505.02965 .  Carl Edward Rasmussen and Christopher K. I. Williams. 2006. Gaussian Processes for Machine Learning. The MIT Press. Online version:  http://www.gaussianprocess.org/gpml/", 
            "title": "Useful Resources"
        }, 
        {
            "location": "/classical-mds/", 
            "text": "classical-mds\n\n\nClassical multi-dimensional scaling (MDS) for dimensionality reduction and low-dimensional embedding. This is also useful for visualizing the similarities of individual items in a 2-dimensional scattered plot.\n\n\nHeader\n\n\n#include \nmathtoolbox/classical-mds.hpp\n\n\n\n\n\nMath\n\n\nOverview\n\n\nGiven a distance (or dissimilarity) matrix of \n n \n elements\n\n\n\n\n\n\\mathbf{D} \\in \\mathbb{R}^{n \\times n}\n\n\n\n\n\nand a target dimensionality \n m \n, this technique calculates a set of \n m \n-dimensional coordinates for them:\n\n\n\n\n\n\\mathbf{X} = \\begin{bmatrix} \\mathbf{x}_1 & \\cdots & \\mathbf{x}_n \\end{bmatrix} \\in \\mathbb{R}^{m \\times n}.\n\n\n\n\n\nIf the elements are originally defined in an \n m' \n-dimensional space (\n m < m' \n) and Euclidian distance is used for calculating the distance matrix, then this is considered dimensionality reduction (or low-dimensional embedding).\n\n\nAlgorithm\n\n\nFirst, calculate the kernel matrix:\n\n\n\n\n\n\\mathbf{K} = - \\frac{1}{2} \\mathbf{H} \\mathbf{D}^{(2)} \\mathbf{H} \\in \\mathbb{R}^{n \\times n},\n\n\n\n\n\nwhere \n \\mathbf{H} \n is called the centering matrix and defined as\n\n\n\n\n\n\\mathbf{H} = \\mathbf{I} - \\frac{1}{n} \\mathbf{1}^T \\mathbf{1} \\in \\mathbb{R}^{n \\times n},\n\n\n\n\n\nand \n \\mathbf{D}^{(2)} \n is the squared distance matrix.\n\n\nThen, apply eigenvalue decomposition to \n \\mathbf{K} \n:\n\n\n\n\n\n\\mathbf{K} = \\mathbf{V} \\mathbf{\\Lambda} \\mathbf{V}^T.\n\n\n\n\n\nFinally, pick up the \n m \n-largest eigenvalues \n \\mathbf{\\Lambda}_m \n and corresponding eigenvectors \n \\mathbf{V}_m \n, and calculate \n \\mathbf{X} \n by\n\n\n\n\n\n\\mathbf{X} = \\mathbf{V}_m \\mathbf{\\Lambda}_m^\\frac{1}{2}.\n\n\n\n\n\nUsage\n\n\nThis technique can be calculated by the following function:\n\n\nEigen::MatrixXd ComputeClassicalMds(const Eigen::MatrixXd\n D, unsigned dim);\n\n\n\n\nwhere \ndim\n is the target dimensionality for embedding.\n\n\nUseful Resources\n\n\n\n\nJosh Wills, Sameer Agarwal, David Kriegman, and Serge Belongie. 2009. Toward a perceptual space for gloss. ACM Trans. Graph. 28, 4, Article 103 (September 2009), 15 pages. DOI: \nhttps://doi.org/10.1145/1559755.1559760", 
            "title": "classical-mds"
        }, 
        {
            "location": "/classical-mds/#classical-mds", 
            "text": "Classical multi-dimensional scaling (MDS) for dimensionality reduction and low-dimensional embedding. This is also useful for visualizing the similarities of individual items in a 2-dimensional scattered plot.", 
            "title": "classical-mds"
        }, 
        {
            "location": "/classical-mds/#header", 
            "text": "#include  mathtoolbox/classical-mds.hpp", 
            "title": "Header"
        }, 
        {
            "location": "/classical-mds/#math", 
            "text": "", 
            "title": "Math"
        }, 
        {
            "location": "/classical-mds/#overview", 
            "text": "Given a distance (or dissimilarity) matrix of   n   elements   \n\\mathbf{D} \\in \\mathbb{R}^{n \\times n}   and a target dimensionality   m  , this technique calculates a set of   m  -dimensional coordinates for them:   \n\\mathbf{X} = \\begin{bmatrix} \\mathbf{x}_1 & \\cdots & \\mathbf{x}_n \\end{bmatrix} \\in \\mathbb{R}^{m \\times n}.   If the elements are originally defined in an   m'  -dimensional space (  m < m'  ) and Euclidian distance is used for calculating the distance matrix, then this is considered dimensionality reduction (or low-dimensional embedding).", 
            "title": "Overview"
        }, 
        {
            "location": "/classical-mds/#algorithm", 
            "text": "First, calculate the kernel matrix:   \n\\mathbf{K} = - \\frac{1}{2} \\mathbf{H} \\mathbf{D}^{(2)} \\mathbf{H} \\in \\mathbb{R}^{n \\times n},   where   \\mathbf{H}   is called the centering matrix and defined as   \n\\mathbf{H} = \\mathbf{I} - \\frac{1}{n} \\mathbf{1}^T \\mathbf{1} \\in \\mathbb{R}^{n \\times n},   and   \\mathbf{D}^{(2)}   is the squared distance matrix.  Then, apply eigenvalue decomposition to   \\mathbf{K}  :   \n\\mathbf{K} = \\mathbf{V} \\mathbf{\\Lambda} \\mathbf{V}^T.   Finally, pick up the   m  -largest eigenvalues   \\mathbf{\\Lambda}_m   and corresponding eigenvectors   \\mathbf{V}_m  , and calculate   \\mathbf{X}   by   \n\\mathbf{X} = \\mathbf{V}_m \\mathbf{\\Lambda}_m^\\frac{1}{2}.", 
            "title": "Algorithm"
        }, 
        {
            "location": "/classical-mds/#usage", 
            "text": "This technique can be calculated by the following function:  Eigen::MatrixXd ComputeClassicalMds(const Eigen::MatrixXd  D, unsigned dim);  where  dim  is the target dimensionality for embedding.", 
            "title": "Usage"
        }, 
        {
            "location": "/classical-mds/#useful-resources", 
            "text": "Josh Wills, Sameer Agarwal, David Kriegman, and Serge Belongie. 2009. Toward a perceptual space for gloss. ACM Trans. Graph. 28, 4, Article 103 (September 2009), 15 pages. DOI:  https://doi.org/10.1145/1559755.1559760", 
            "title": "Useful Resources"
        }
    ]
}