{"config":{"lang":["en"],"prebuild_index":false,"separator":"[\\s\\-]+"},"docs":[{"location":"","text":"mathtoolbox A library of mathematical tools (regression, interpolation, dimensionality reduction, clustering, etc.) written in C++11 and Eigen . Algorithms Scattered Data Interpolation and Function Approximation rbf-interpolation : Radial basis function (RBF) network gaussian-process-regression : Gaussian process regression (GPR) Dimensionality Reduction and Low-Dimensional Embedding classical-mds : Classical multi-dimensional scaling (MDS) Numerical Optimization backtracking-line-search : Backtracking line search bfgs : BFGS method l-bfgs : Limited-memory BFGS method strong-wolfe-conditions-line-search : Strong Wolfe conditions line search Utilities acquisition-functions : Acquisition functions constants : Constants kernel-functions : Kernel functions probability-distributions : Probability distributions Dependencies Main Library Eigen http://eigen.tuxfamily.org/ ( brew install eigen ) Python Bindings pybind11 https://github.com/pybind/pybind11 (included as gitsubmodule) Examples optimization-test-function https://github.com/yuki-koyama/optimization-test-functions (included as gitsubmodule) Build and Installation mathtoolbox uses CMake https://cmake.org/ for building source codes. This library can be built, for example, by git clone https://github.com/yuki-koyama/mathtoolbox.git --recursive cd mathtoolbox mkdir build cd build cmake ../ make and optionally it can be installed to the system by make install When the CMake parameter MATHTOOLBOX_BUILD_EXAMPLES is set ON , the example applications are also built. (The default setting is OFF .) This is done by cmake ../ -DMATHTOOLBOX_BUILD_EXAMPLES=ON make When the CMake parameter MATHTOOLBOX_PYTHON_BINDINGS is set ON , the example applications are also built. (The default setting is OFF .) This is done by cmake ../ -DMATHTOOLBOX_PYTHON_BINDINGS=ON make Prerequisites macOS: brew install eigen Ubuntu: sudo apt install libeigen3-dev Projects Using mathtoolbox SelPh https://github.com/yuki-koyama/selph (for classical-mds ) Sequential Line Search https://github.com/yuki-koyama/sequential-line-search (for acquisition-functions , kernel-functions , and probability-distributions ) Licensing The MIT License.","title":"Introduction"},{"location":"#mathtoolbox","text":"A library of mathematical tools (regression, interpolation, dimensionality reduction, clustering, etc.) written in C++11 and Eigen .","title":"mathtoolbox"},{"location":"#algorithms","text":"","title":"Algorithms"},{"location":"#scattered-data-interpolation-and-function-approximation","text":"rbf-interpolation : Radial basis function (RBF) network gaussian-process-regression : Gaussian process regression (GPR)","title":"Scattered Data Interpolation and Function Approximation"},{"location":"#dimensionality-reduction-and-low-dimensional-embedding","text":"classical-mds : Classical multi-dimensional scaling (MDS)","title":"Dimensionality Reduction and Low-Dimensional Embedding"},{"location":"#numerical-optimization","text":"backtracking-line-search : Backtracking line search bfgs : BFGS method l-bfgs : Limited-memory BFGS method strong-wolfe-conditions-line-search : Strong Wolfe conditions line search","title":"Numerical Optimization"},{"location":"#utilities","text":"acquisition-functions : Acquisition functions constants : Constants kernel-functions : Kernel functions probability-distributions : Probability distributions","title":"Utilities"},{"location":"#dependencies","text":"","title":"Dependencies"},{"location":"#main-library","text":"Eigen http://eigen.tuxfamily.org/ ( brew install eigen )","title":"Main Library"},{"location":"#python-bindings","text":"pybind11 https://github.com/pybind/pybind11 (included as gitsubmodule)","title":"Python Bindings"},{"location":"#examples","text":"optimization-test-function https://github.com/yuki-koyama/optimization-test-functions (included as gitsubmodule)","title":"Examples"},{"location":"#build-and-installation","text":"mathtoolbox uses CMake https://cmake.org/ for building source codes. This library can be built, for example, by git clone https://github.com/yuki-koyama/mathtoolbox.git --recursive cd mathtoolbox mkdir build cd build cmake ../ make and optionally it can be installed to the system by make install When the CMake parameter MATHTOOLBOX_BUILD_EXAMPLES is set ON , the example applications are also built. (The default setting is OFF .) This is done by cmake ../ -DMATHTOOLBOX_BUILD_EXAMPLES=ON make When the CMake parameter MATHTOOLBOX_PYTHON_BINDINGS is set ON , the example applications are also built. (The default setting is OFF .) This is done by cmake ../ -DMATHTOOLBOX_PYTHON_BINDINGS=ON make","title":"Build and Installation"},{"location":"#prerequisites","text":"macOS: brew install eigen Ubuntu: sudo apt install libeigen3-dev","title":"Prerequisites"},{"location":"#projects-using-mathtoolbox","text":"SelPh https://github.com/yuki-koyama/selph (for classical-mds ) Sequential Line Search https://github.com/yuki-koyama/sequential-line-search (for acquisition-functions , kernel-functions , and probability-distributions )","title":"Projects Using mathtoolbox"},{"location":"#licensing","text":"The MIT License.","title":"Licensing"},{"location":"acquisition-functions/","text":"acquisition-functions Acquisition functions for Bayesian optimization Header #include <mathtoolbox/acquisition-functions.hpp> Overview The following acquisition functions are supported: Expected improvement (EI) Useful Resources Jasper Snoek, Hugo Larochelle, and Ryan P. Adams. 2012. Practical Bayesian optimization of machine learning algorithms. In Proc. NIPS '12, pp.2951--2959. Yuki Koyama, Issei Sato, Daisuke Sakamoto, and Takeo Igarashi. 2017. Sequential line search for efficient visual design optimization by crowds. ACM Trans. Graph. 36, 4, pp.48:1--48:11 (2017). DOI: https://doi.org/10.1145/3072959.3073598","title":"acquisition-functions"},{"location":"acquisition-functions/#acquisition-functions","text":"Acquisition functions for Bayesian optimization","title":"acquisition-functions"},{"location":"acquisition-functions/#header","text":"#include <mathtoolbox/acquisition-functions.hpp>","title":"Header"},{"location":"acquisition-functions/#overview","text":"The following acquisition functions are supported: Expected improvement (EI)","title":"Overview"},{"location":"acquisition-functions/#useful-resources","text":"Jasper Snoek, Hugo Larochelle, and Ryan P. Adams. 2012. Practical Bayesian optimization of machine learning algorithms. In Proc. NIPS '12, pp.2951--2959. Yuki Koyama, Issei Sato, Daisuke Sakamoto, and Takeo Igarashi. 2017. Sequential line search for efficient visual design optimization by crowds. ACM Trans. Graph. 36, 4, pp.48:1--48:11 (2017). DOI: https://doi.org/10.1145/3072959.3073598","title":"Useful Resources"},{"location":"backtracking-line-search/","text":"backtracking-line-search A line search method for finding a step size that satisfies the Armijo (i.e., sufficient decrease) condition based on a simple backtracking procedure. Header #include <mathtoolbox/backtracking-line-search.hpp> Math and Algorithm We follow Nocedal and Wright (2006) (Chapter 3, specifically Algorithm 3.1). Useful Resources Jorge Nocedal and Stephen J. Wright. 2006. Numerical optimization (2nd ed.). Springer. DOI: https://doi.org/10.1007/978-0-387-40065-5","title":"backtracking-line-search"},{"location":"backtracking-line-search/#backtracking-line-search","text":"A line search method for finding a step size that satisfies the Armijo (i.e., sufficient decrease) condition based on a simple backtracking procedure.","title":"backtracking-line-search"},{"location":"backtracking-line-search/#header","text":"#include <mathtoolbox/backtracking-line-search.hpp>","title":"Header"},{"location":"backtracking-line-search/#math-and-algorithm","text":"We follow Nocedal and Wright (2006) (Chapter 3, specifically Algorithm 3.1).","title":"Math and Algorithm"},{"location":"backtracking-line-search/#useful-resources","text":"Jorge Nocedal and Stephen J. Wright. 2006. Numerical optimization (2nd ed.). Springer. DOI: https://doi.org/10.1007/978-0-387-40065-5","title":"Useful Resources"},{"location":"bfgs/","text":"bfgs The BFGS method (BFGS) is a numerical optimization algorithm that is one of the most popular choices among quasi-Newton methods. Header #include <mathtoolbox/bfgs.hpp> Internal Dependencies strong-wolfe-conditions-line-search Math and Algorithm We follow Nocedal and Wright (2006) (Chapter 6). Inverse Hessian Initialization This implementation adopts the strategy described in Equation 6.20: \\mathbf{H}_0 \\leftarrow \\frac{\\mathbf{y}_k^T \\mathbf{s}_k}{\\mathbf{y}_k^T \\mathbf{y}_k} \\mathbf{I}. See the book for details. Line Search This implementation uses strong-wolfe-conditions-line-search to find an appropriate step size. Useful Resources Jorge Nocedal and Stephen J. Wright. 2006. Numerical optimization (2nd ed.). Springer. DOI: https://doi.org/10.1007/978-0-387-40065-5","title":"bfgs"},{"location":"bfgs/#bfgs","text":"The BFGS method (BFGS) is a numerical optimization algorithm that is one of the most popular choices among quasi-Newton methods.","title":"bfgs"},{"location":"bfgs/#header","text":"#include <mathtoolbox/bfgs.hpp>","title":"Header"},{"location":"bfgs/#internal-dependencies","text":"strong-wolfe-conditions-line-search","title":"Internal Dependencies"},{"location":"bfgs/#math-and-algorithm","text":"We follow Nocedal and Wright (2006) (Chapter 6).","title":"Math and Algorithm"},{"location":"bfgs/#inverse-hessian-initialization","text":"This implementation adopts the strategy described in Equation 6.20: \\mathbf{H}_0 \\leftarrow \\frac{\\mathbf{y}_k^T \\mathbf{s}_k}{\\mathbf{y}_k^T \\mathbf{y}_k} \\mathbf{I}. See the book for details.","title":"Inverse Hessian Initialization"},{"location":"bfgs/#line-search","text":"This implementation uses strong-wolfe-conditions-line-search to find an appropriate step size.","title":"Line Search"},{"location":"bfgs/#useful-resources","text":"Jorge Nocedal and Stephen J. Wright. 2006. Numerical optimization (2nd ed.). Springer. DOI: https://doi.org/10.1007/978-0-387-40065-5","title":"Useful Resources"},{"location":"classical-mds/","text":"classical-mds Classical multi-dimensional scaling (MDS) for dimensionality reduction and low-dimensional embedding. This is also useful for visualizing the similarities of individual items in a 2-dimensional scattered plot. Header #include <mathtoolbox/classical-mds.hpp> Math Overview Given a distance (or dissimilarity) matrix of n elements \\mathbf{D} \\in \\mathbb{R}^{n \\times n} and a target dimensionality m , this technique calculates a set of m -dimensional coordinates for them: \\mathbf{X} = \\begin{bmatrix} \\mathbf{x}_1 & \\cdots & \\mathbf{x}_n \\end{bmatrix} \\in \\mathbb{R}^{m \\times n}. If the elements are originally defined in an m' -dimensional space ( m < m' ) and Euclidian distance is used for calculating the distance matrix, then this is considered dimensionality reduction (or low-dimensional embedding). Algorithm First, calculate the kernel matrix: \\mathbf{K} = - \\frac{1}{2} \\mathbf{H} \\mathbf{D}^{(2)} \\mathbf{H} \\in \\mathbb{R}^{n \\times n}, where \\mathbf{H} is called the centering matrix and defined as \\mathbf{H} = \\mathbf{I} - \\frac{1}{n} \\mathbf{1}^T \\mathbf{1} \\in \\mathbb{R}^{n \\times n}, and \\mathbf{D}^{(2)} is the squared distance matrix. Then, apply eigenvalue decomposition to \\mathbf{K} : \\mathbf{K} = \\mathbf{V} \\mathbf{\\Lambda} \\mathbf{V}^T. Finally, pick up the m -largest eigenvalues \\mathbf{\\Lambda}_m and corresponding eigenvectors \\mathbf{V}_m , and calculate \\mathbf{X} by \\mathbf{X} = \\mathbf{V}_m \\mathbf{\\Lambda}_m^\\frac{1}{2}. Usage This technique can be calculated by the following function: Eigen::MatrixXd ComputeClassicalMds(const Eigen::MatrixXd& D, unsigned dim); where dim is the target dimensionality for embedding. Useful Resources Josh Wills, Sameer Agarwal, David Kriegman, and Serge Belongie. 2009. Toward a perceptual space for gloss. ACM Trans. Graph. 28, 4, Article 103 (September 2009), 15 pages. DOI: https://doi.org/10.1145/1559755.1559760","title":"classical-mds"},{"location":"classical-mds/#classical-mds","text":"Classical multi-dimensional scaling (MDS) for dimensionality reduction and low-dimensional embedding. This is also useful for visualizing the similarities of individual items in a 2-dimensional scattered plot.","title":"classical-mds"},{"location":"classical-mds/#header","text":"#include <mathtoolbox/classical-mds.hpp>","title":"Header"},{"location":"classical-mds/#math","text":"","title":"Math"},{"location":"classical-mds/#overview","text":"Given a distance (or dissimilarity) matrix of n elements \\mathbf{D} \\in \\mathbb{R}^{n \\times n} and a target dimensionality m , this technique calculates a set of m -dimensional coordinates for them: \\mathbf{X} = \\begin{bmatrix} \\mathbf{x}_1 & \\cdots & \\mathbf{x}_n \\end{bmatrix} \\in \\mathbb{R}^{m \\times n}. If the elements are originally defined in an m' -dimensional space ( m < m' ) and Euclidian distance is used for calculating the distance matrix, then this is considered dimensionality reduction (or low-dimensional embedding).","title":"Overview"},{"location":"classical-mds/#algorithm","text":"First, calculate the kernel matrix: \\mathbf{K} = - \\frac{1}{2} \\mathbf{H} \\mathbf{D}^{(2)} \\mathbf{H} \\in \\mathbb{R}^{n \\times n}, where \\mathbf{H} is called the centering matrix and defined as \\mathbf{H} = \\mathbf{I} - \\frac{1}{n} \\mathbf{1}^T \\mathbf{1} \\in \\mathbb{R}^{n \\times n}, and \\mathbf{D}^{(2)} is the squared distance matrix. Then, apply eigenvalue decomposition to \\mathbf{K} : \\mathbf{K} = \\mathbf{V} \\mathbf{\\Lambda} \\mathbf{V}^T. Finally, pick up the m -largest eigenvalues \\mathbf{\\Lambda}_m and corresponding eigenvectors \\mathbf{V}_m , and calculate \\mathbf{X} by \\mathbf{X} = \\mathbf{V}_m \\mathbf{\\Lambda}_m^\\frac{1}{2}.","title":"Algorithm"},{"location":"classical-mds/#usage","text":"This technique can be calculated by the following function: Eigen::MatrixXd ComputeClassicalMds(const Eigen::MatrixXd& D, unsigned dim); where dim is the target dimensionality for embedding.","title":"Usage"},{"location":"classical-mds/#useful-resources","text":"Josh Wills, Sameer Agarwal, David Kriegman, and Serge Belongie. 2009. Toward a perceptual space for gloss. ACM Trans. Graph. 28, 4, Article 103 (September 2009), 15 pages. DOI: https://doi.org/10.1145/1559755.1559760","title":"Useful Resources"},{"location":"constants/","text":"constants Constants used for various mathematical contexts. Header #include <mathtoolbox/constants.hpp>","title":"constants"},{"location":"constants/#constants","text":"Constants used for various mathematical contexts.","title":"constants"},{"location":"constants/#header","text":"#include <mathtoolbox/constants.hpp>","title":"Header"},{"location":"gaussian-process-regression/","text":"gaussian-process-regression Gaussian process regression (GPR) for scattered data interpolation and function approximation. Header #include <mathtoolbox/gaussian-process-regression.hpp> Overview Input The input consists of a set of N scattered data points: \\{ (\\mathbf{x}_i, y_i) \\}_{i = 1, \\ldots, N}, where \\mathbf{x}_i \\in \\mathbb{R}^D is the i -th data point location in a D -dimensional space and y_i \\in \\mathbb{R} is its associated value. This input data is also denoted as \\mathbf{X} = \\begin{bmatrix} \\mathbf{x}_{1} & \\cdots & \\mathbf{x}_{N} \\end{bmatrix} \\in \\mathbb{R}^{D \\times N} and \\mathbf{y} = \\begin{bmatrix} y_1 \\\\ \\vdots \\\\ y_N \\end{bmatrix} \\in \\mathbb{R}^{N}. Output Given the data and some \"Gaussian process\" assumptions, GPR can calculate the most likely value y and its variance \\text{Var}(y) for an arbitrary location \\mathbf{x} . The variance roughly indicates how uncertain the estimation is. For example, when this value is large, the estimated value may not be very trustful (this often occurs in regions with less data points). Note that a 95%-confidence interval can be obtained by [ y - 1.96 \\sqrt{\\text{Var}(y)}, y + 1.96 \\sqrt{\\text{Var}(y)} ] . Math Coveriance Function The automatic relevance determination (ARD) squared exponential kernel is used: k(\\mathbf{x}_p, \\mathbf{x}_q) = \\sigma_f^{2} \\exp \\left( - \\frac{1}{2} (\\mathbf{x}_p - \\mathbf{x}_q)^{T} \\text{diag}(\\boldsymbol{\\ell})^{-2} (\\mathbf{x}_p - \\mathbf{x}_q) \\right) + \\sigma_n^{2} \\delta_{pq}, where \\sigma_f^{2} (the signal variance), \\sigma_n^{2} (the noise level), and \\boldsymbol{\\ell} (the characteristic length-scales) are hyperparameters. Mean Function A constant-value function is used: m(\\mathbf{x}) = 0. Selecting Hyperparameters There are two options for setting hyperparameters: Set manually Determined by the maximum likelihood estimation Maximum Likelihood Estimation Let \\boldsymbol{\\theta} be a concatenation of hyperparameters; that is, \\boldsymbol{\\theta} = \\begin{bmatrix} \\sigma_{f}^{2} \\\\ \\sigma_{n}^{2} \\\\ \\boldsymbol{\\ell} \\end{bmatrix} \\in \\mathbb{R}^{D + 2}. In this approach, these hyperparameters are determined by solving the following numerical optimization problem: \\boldsymbol{\\theta}^\\text{ML} = \\mathop{\\rm arg~max}\\limits_{\\boldsymbol{\\theta}} p(\\mathbf{y} \\mid \\mathbf{X}, \\boldsymbol{\\theta}). In this implementation, this maximization problem is solved by the L-BFGS method (a gradient-based local optimization algorithm) from the NLopt library https://nlopt.readthedocs.io/ . Initial solutions for this maximization need to be specified. Usage Instantiation and Data Specification A GPR object is instantiated with data specification in its constructor: GaussianProcessRegression(const Eigen::MatrixXd& X, const Eigen::VectorXd& y); Hyperparameter Selection Hyperparameters are set by either void SetHyperparameters(double sigma_squared_f, double sigma_squared_n, const Eigen::VectorXd& length_scales); or void PerformMaximumLikelihood(double sigma_squared_f_initial, double sigma_squared_n_initial, const Eigen::VectorXd& length_scales_initial); Estimation Once a GPR object is instantiated and its hyperparameters are set, it is ready for estimation. For an unknown location \\mathbf{x} , the GPR object estimates the most likely value y by the following method: double EstimateY(const Eigen::VectorXd& x) const; It also estimates the variance \\text{Var}(y) by the following method: double EstimateVariance(const Eigen::VectorXd& x) const; Useful Resources Mark Ebden. 2015. Gaussian Processes: A Quick Introduction. arXiv:1505.02965 . Carl Edward Rasmussen and Christopher K. I. Williams. 2006. Gaussian Processes for Machine Learning. The MIT Press. Online version: http://www.gaussianprocess.org/gpml/","title":"gaussian-process-regression"},{"location":"gaussian-process-regression/#gaussian-process-regression","text":"Gaussian process regression (GPR) for scattered data interpolation and function approximation.","title":"gaussian-process-regression"},{"location":"gaussian-process-regression/#header","text":"#include <mathtoolbox/gaussian-process-regression.hpp>","title":"Header"},{"location":"gaussian-process-regression/#overview","text":"","title":"Overview"},{"location":"gaussian-process-regression/#input","text":"The input consists of a set of N scattered data points: \\{ (\\mathbf{x}_i, y_i) \\}_{i = 1, \\ldots, N}, where \\mathbf{x}_i \\in \\mathbb{R}^D is the i -th data point location in a D -dimensional space and y_i \\in \\mathbb{R} is its associated value. This input data is also denoted as \\mathbf{X} = \\begin{bmatrix} \\mathbf{x}_{1} & \\cdots & \\mathbf{x}_{N} \\end{bmatrix} \\in \\mathbb{R}^{D \\times N} and \\mathbf{y} = \\begin{bmatrix} y_1 \\\\ \\vdots \\\\ y_N \\end{bmatrix} \\in \\mathbb{R}^{N}.","title":"Input"},{"location":"gaussian-process-regression/#output","text":"Given the data and some \"Gaussian process\" assumptions, GPR can calculate the most likely value y and its variance \\text{Var}(y) for an arbitrary location \\mathbf{x} . The variance roughly indicates how uncertain the estimation is. For example, when this value is large, the estimated value may not be very trustful (this often occurs in regions with less data points). Note that a 95%-confidence interval can be obtained by [ y - 1.96 \\sqrt{\\text{Var}(y)}, y + 1.96 \\sqrt{\\text{Var}(y)} ] .","title":"Output"},{"location":"gaussian-process-regression/#math","text":"","title":"Math"},{"location":"gaussian-process-regression/#coveriance-function","text":"The automatic relevance determination (ARD) squared exponential kernel is used: k(\\mathbf{x}_p, \\mathbf{x}_q) = \\sigma_f^{2} \\exp \\left( - \\frac{1}{2} (\\mathbf{x}_p - \\mathbf{x}_q)^{T} \\text{diag}(\\boldsymbol{\\ell})^{-2} (\\mathbf{x}_p - \\mathbf{x}_q) \\right) + \\sigma_n^{2} \\delta_{pq}, where \\sigma_f^{2} (the signal variance), \\sigma_n^{2} (the noise level), and \\boldsymbol{\\ell} (the characteristic length-scales) are hyperparameters.","title":"Coveriance Function"},{"location":"gaussian-process-regression/#mean-function","text":"A constant-value function is used: m(\\mathbf{x}) = 0.","title":"Mean Function"},{"location":"gaussian-process-regression/#selecting-hyperparameters","text":"There are two options for setting hyperparameters: Set manually Determined by the maximum likelihood estimation","title":"Selecting Hyperparameters"},{"location":"gaussian-process-regression/#maximum-likelihood-estimation","text":"Let \\boldsymbol{\\theta} be a concatenation of hyperparameters; that is, \\boldsymbol{\\theta} = \\begin{bmatrix} \\sigma_{f}^{2} \\\\ \\sigma_{n}^{2} \\\\ \\boldsymbol{\\ell} \\end{bmatrix} \\in \\mathbb{R}^{D + 2}. In this approach, these hyperparameters are determined by solving the following numerical optimization problem: \\boldsymbol{\\theta}^\\text{ML} = \\mathop{\\rm arg~max}\\limits_{\\boldsymbol{\\theta}} p(\\mathbf{y} \\mid \\mathbf{X}, \\boldsymbol{\\theta}). In this implementation, this maximization problem is solved by the L-BFGS method (a gradient-based local optimization algorithm) from the NLopt library https://nlopt.readthedocs.io/ . Initial solutions for this maximization need to be specified.","title":"Maximum Likelihood Estimation"},{"location":"gaussian-process-regression/#usage","text":"","title":"Usage"},{"location":"gaussian-process-regression/#instantiation-and-data-specification","text":"A GPR object is instantiated with data specification in its constructor: GaussianProcessRegression(const Eigen::MatrixXd& X, const Eigen::VectorXd& y);","title":"Instantiation and Data Specification"},{"location":"gaussian-process-regression/#hyperparameter-selection","text":"Hyperparameters are set by either void SetHyperparameters(double sigma_squared_f, double sigma_squared_n, const Eigen::VectorXd& length_scales); or void PerformMaximumLikelihood(double sigma_squared_f_initial, double sigma_squared_n_initial, const Eigen::VectorXd& length_scales_initial);","title":"Hyperparameter Selection"},{"location":"gaussian-process-regression/#estimation","text":"Once a GPR object is instantiated and its hyperparameters are set, it is ready for estimation. For an unknown location \\mathbf{x} , the GPR object estimates the most likely value y by the following method: double EstimateY(const Eigen::VectorXd& x) const; It also estimates the variance \\text{Var}(y) by the following method: double EstimateVariance(const Eigen::VectorXd& x) const;","title":"Estimation"},{"location":"gaussian-process-regression/#useful-resources","text":"Mark Ebden. 2015. Gaussian Processes: A Quick Introduction. arXiv:1505.02965 . Carl Edward Rasmussen and Christopher K. I. Williams. 2006. Gaussian Processes for Machine Learning. The MIT Press. Online version: http://www.gaussianprocess.org/gpml/","title":"Useful Resources"},{"location":"kernel-functions/","text":"kernel-functions Kernel functions for various techniques. Header #include <mathtoolbox/kernel-functions.hpp> Overview Automatic Relevance Determination (ARD) Squared Exponential Kernel This kernel is defined as k(\\mathbf{x}_p, \\mathbf{x}_q ; \\boldsymbol{\\theta}) = \\sigma_f^{2} \\exp \\left( - \\frac{1}{2} (\\mathbf{x}_p - \\mathbf{x}_q)^{T} \\text{diag}(\\boldsymbol{\\ell})^{-2} (\\mathbf{x}_p - \\mathbf{x}_q) \\right), where \\sigma_f^{2} (the signal variance) and \\boldsymbol{\\ell} (the characteristic length-scales) are its hyperparameters. That is, \\boldsymbol{\\theta} = \\begin{bmatrix} \\sigma_{f}^{2} \\\\ \\boldsymbol{\\ell} \\end{bmatrix} \\in \\mathbb{R}^{n + 1}_{> 0}. Automatic Relevance Determination (ARD) Matern 5/2 Kernel This kernel is defined as k(\\mathbf{x}_p, \\mathbf{x}_q ; \\boldsymbol{\\theta}) = \\sigma_f^{2} \\left( 1 + \\sqrt{5 r^{2}(\\mathbf{x}_{p}, \\mathbf{x}_{q}; \\boldsymbol{\\ell})} + \\frac{5}{3} r^{2}(\\mathbf{x}_{p}, \\mathbf{x}_{q}; \\boldsymbol{\\ell}) \\right) \\exp \\left\\{ - \\sqrt{5 r^{2}(\\mathbf{x}_{p}, \\mathbf{x}_{q}; \\boldsymbol{\\ell})} \\right\\}, where r^{2}(\\mathbf{x}_{p}, \\mathbf{x}_{q}; \\boldsymbol{\\ell}) = (\\mathbf{x}_p - \\mathbf{x}_q)^{T} \\text{diag}(\\boldsymbol{\\ell})^{-2} (\\mathbf{x}_p - \\mathbf{x}_q) and \\sigma_f^{2} (the signal variance) and \\boldsymbol{\\ell} (the characteristic length-scales) are its hyperparameters. That is, \\boldsymbol{\\theta} = \\begin{bmatrix} \\sigma_{f}^{2} \\\\ \\boldsymbol{\\ell} \\end{bmatrix} \\in \\mathbb{R}^{n + 1}_{> 0}. Useful Resources Carl Edward Rasmussen and Christopher K. I. Williams. 2006. Gaussian Processes for Machine Learning. The MIT Press. Online version: http://www.gaussianprocess.org/gpml/","title":"kernel-functions"},{"location":"kernel-functions/#kernel-functions","text":"Kernel functions for various techniques.","title":"kernel-functions"},{"location":"kernel-functions/#header","text":"#include <mathtoolbox/kernel-functions.hpp>","title":"Header"},{"location":"kernel-functions/#overview","text":"","title":"Overview"},{"location":"kernel-functions/#automatic-relevance-determination-ard-squared-exponential-kernel","text":"This kernel is defined as k(\\mathbf{x}_p, \\mathbf{x}_q ; \\boldsymbol{\\theta}) = \\sigma_f^{2} \\exp \\left( - \\frac{1}{2} (\\mathbf{x}_p - \\mathbf{x}_q)^{T} \\text{diag}(\\boldsymbol{\\ell})^{-2} (\\mathbf{x}_p - \\mathbf{x}_q) \\right), where \\sigma_f^{2} (the signal variance) and \\boldsymbol{\\ell} (the characteristic length-scales) are its hyperparameters. That is, \\boldsymbol{\\theta} = \\begin{bmatrix} \\sigma_{f}^{2} \\\\ \\boldsymbol{\\ell} \\end{bmatrix} \\in \\mathbb{R}^{n + 1}_{> 0}.","title":"Automatic Relevance Determination (ARD) Squared Exponential Kernel"},{"location":"kernel-functions/#automatic-relevance-determination-ard-matern-52-kernel","text":"This kernel is defined as k(\\mathbf{x}_p, \\mathbf{x}_q ; \\boldsymbol{\\theta}) = \\sigma_f^{2} \\left( 1 + \\sqrt{5 r^{2}(\\mathbf{x}_{p}, \\mathbf{x}_{q}; \\boldsymbol{\\ell})} + \\frac{5}{3} r^{2}(\\mathbf{x}_{p}, \\mathbf{x}_{q}; \\boldsymbol{\\ell}) \\right) \\exp \\left\\{ - \\sqrt{5 r^{2}(\\mathbf{x}_{p}, \\mathbf{x}_{q}; \\boldsymbol{\\ell})} \\right\\}, where r^{2}(\\mathbf{x}_{p}, \\mathbf{x}_{q}; \\boldsymbol{\\ell}) = (\\mathbf{x}_p - \\mathbf{x}_q)^{T} \\text{diag}(\\boldsymbol{\\ell})^{-2} (\\mathbf{x}_p - \\mathbf{x}_q) and \\sigma_f^{2} (the signal variance) and \\boldsymbol{\\ell} (the characteristic length-scales) are its hyperparameters. That is, \\boldsymbol{\\theta} = \\begin{bmatrix} \\sigma_{f}^{2} \\\\ \\boldsymbol{\\ell} \\end{bmatrix} \\in \\mathbb{R}^{n + 1}_{> 0}.","title":"Automatic Relevance Determination (ARD) Matern 5/2 Kernel"},{"location":"kernel-functions/#useful-resources","text":"Carl Edward Rasmussen and Christopher K. I. Williams. 2006. Gaussian Processes for Machine Learning. The MIT Press. Online version: http://www.gaussianprocess.org/gpml/","title":"Useful Resources"},{"location":"l-bfgs/","text":"l-bfgs The Limited-memory BFGS method (L-BFGS) is a numerical optimization algorithm that is one of the most popular choices among quasi-Newton methods. Header #include <mathtoolbox/l-bfgs.hpp> Internal Dependencies strong-wolfe-conditions-line-search Math and Algorithm We follow Nocedal and Wright (2006) (Chapter 7). Inverse Hessian Initialization This implementation adopts the strategy described in Equation 7.20: \\mathbf{H}_k^0 \\leftarrow \\frac{\\mathbf{y}_{k - 1}^T \\mathbf{s}_{k - 1}}{\\mathbf{y}_{k - 1}^T \\mathbf{y}_{k - 1}} \\mathbf{I}. See the book for details. Line Search This implementation uses strong-wolfe-conditions-line-search to find an appropriate step size. Useful Resources Jorge Nocedal and Stephen J. Wright. 2006. Numerical optimization (2nd ed.). Springer. DOI: https://doi.org/10.1007/978-0-387-40065-5","title":"l-bfgs"},{"location":"l-bfgs/#l-bfgs","text":"The Limited-memory BFGS method (L-BFGS) is a numerical optimization algorithm that is one of the most popular choices among quasi-Newton methods.","title":"l-bfgs"},{"location":"l-bfgs/#header","text":"#include <mathtoolbox/l-bfgs.hpp>","title":"Header"},{"location":"l-bfgs/#internal-dependencies","text":"strong-wolfe-conditions-line-search","title":"Internal Dependencies"},{"location":"l-bfgs/#math-and-algorithm","text":"We follow Nocedal and Wright (2006) (Chapter 7).","title":"Math and Algorithm"},{"location":"l-bfgs/#inverse-hessian-initialization","text":"This implementation adopts the strategy described in Equation 7.20: \\mathbf{H}_k^0 \\leftarrow \\frac{\\mathbf{y}_{k - 1}^T \\mathbf{s}_{k - 1}}{\\mathbf{y}_{k - 1}^T \\mathbf{y}_{k - 1}} \\mathbf{I}. See the book for details.","title":"Inverse Hessian Initialization"},{"location":"l-bfgs/#line-search","text":"This implementation uses strong-wolfe-conditions-line-search to find an appropriate step size.","title":"Line Search"},{"location":"l-bfgs/#useful-resources","text":"Jorge Nocedal and Stephen J. Wright. 2006. Numerical optimization (2nd ed.). Springer. DOI: https://doi.org/10.1007/978-0-387-40065-5","title":"Useful Resources"},{"location":"probability-distributions/","text":"probability-distributions Probability distributions for statistical estimation. Header #include <mathtoolbox/probability-distributions.hpp> Overview The following probability distributions and their first derivatives are supported: Standard normal distribution: \\mathcal{N}(x \\mid 0, 1) Normal distribution: \\mathcal{N}(x \\mid \\mu, \\sigma^{2}) Log-normal distribution: \\mathcal{LN}(x \\mid \\mu, \\sigma^{2}) In statistical estimation, taking logarithms of probabilities is often necessary. For this purpose, the following probability distributions and their derivatives are supported: Log of log-normal distribution: \\log \\{ \\mathcal{LN}(x \\mid \\mu, \\sigma^{2}) \\} Useful Resources Normal distribution - Wikipedia. https://en.wikipedia.org/wiki/Normal_distribution . Log-normal distribution - Wikipedia. https://en.wikipedia.org/wiki/Log-normal_distribution .","title":"probability-distributions"},{"location":"probability-distributions/#probability-distributions","text":"Probability distributions for statistical estimation.","title":"probability-distributions"},{"location":"probability-distributions/#header","text":"#include <mathtoolbox/probability-distributions.hpp>","title":"Header"},{"location":"probability-distributions/#overview","text":"The following probability distributions and their first derivatives are supported: Standard normal distribution: \\mathcal{N}(x \\mid 0, 1) Normal distribution: \\mathcal{N}(x \\mid \\mu, \\sigma^{2}) Log-normal distribution: \\mathcal{LN}(x \\mid \\mu, \\sigma^{2}) In statistical estimation, taking logarithms of probabilities is often necessary. For this purpose, the following probability distributions and their derivatives are supported: Log of log-normal distribution: \\log \\{ \\mathcal{LN}(x \\mid \\mu, \\sigma^{2}) \\}","title":"Overview"},{"location":"probability-distributions/#useful-resources","text":"Normal distribution - Wikipedia. https://en.wikipedia.org/wiki/Normal_distribution . Log-normal distribution - Wikipedia. https://en.wikipedia.org/wiki/Log-normal_distribution .","title":"Useful Resources"},{"location":"rbf-interpolation/","text":"rbf-interpolation Radial basis function (RBF) network for scattered data interpolation and function approximation. Header #include <mathtoolbox/rbf-interpolation.hpp> Math Overview Given input data: \\{ (\\mathbf{x}_i, y_i) \\}_{i = 1, \\ldots, n}, this technique calculates an interpolated value y for a specified point \\mathbf{x} by y = f(\\mathbf{x}) = \\sum_{i = 1}^{n} w_{i} \\phi( \\| \\mathbf{x} - \\mathbf{x}_{i} \\|), where \\phi(\\cdot) is a user-selected RBF and w_1, \\cdots, w_n are weights that are calculated in pre-computation. Pre-Computation The weight values need to be calculated in pre-computation. Let \\mathbf{w} = \\begin{bmatrix} w_1 & \\cdots & w_n \\end{bmatrix}^T and \\mathbf{\\Phi} = \\begin{bmatrix} \\phi_{1, 1} & \\cdots & \\phi_{1, n} \\\\ \\vdots & \\ddots & \\vdots \\\\ \\phi_{n, 1} & \\cdots & \\phi_{n, n} \\end{bmatrix}, where \\phi_{i, j} = \\phi(\\| \\mathbf{x}_i - \\mathbf{x}_j \\|). The following linear system is solved for \\mathbf{w} : \\mathbf{\\Phi} \\mathbf{w} = \\mathbf{y}. LU decomposition can be used for solving this problem. Pre-Computation with Regularization The original formulation above is not robust (i.e., overfitting can occur) when the data points are dense and noisy. For such scenarios, it is possible to add a regularization term into pre-computation. That is, the following minimization problem is solved: \\min_{\\mathbf{w}} \\left\\{ \\| \\mathbf{\\Phi} \\mathbf{w} - \\mathbf{y} \\|^2 + \\lambda \\| \\mathbf{w} \\|^2 \\right\\}. The derivative of this objective function with respect to \\mathbf{w} is \\begin{eqnarray*} && \\frac{\\partial}{\\partial \\mathbf{w}} \\left\\{ \\| \\mathbf{\\Phi} \\mathbf{w} - \\mathbf{y} \\|^2 + \\lambda \\| \\mathbf{w} \\|^2 \\right\\} \\\\ &=& \\frac{\\partial}{\\partial \\mathbf{w}} \\| \\mathbf{\\Phi} \\mathbf{w} - \\mathbf{y} \\|^2 + \\lambda \\frac{\\partial}{\\partial \\mathbf{w}} \\| \\mathbf{w} \\|^2 \\\\ &=& 2 \\mathbf{\\Phi}^T (\\mathbf{\\Phi} \\mathbf{w} - \\mathbf{y}) + 2 \\lambda \\mathbf{w} \\\\ &=& 2 \\left\\{ (\\mathbf{\\Phi}^T \\mathbf{\\Phi} + \\lambda \\mathbf{I}) \\mathbf{w} - \\mathbf{\\Phi}^T \\mathbf{y} \\right\\}. \\end{eqnarray*} Thus, the solution of the above minimization problem can be obtained by solving the below linear system: (\\mathbf{\\Phi}^T \\mathbf{\\Phi} + \\lambda \\mathbf{I}) \\mathbf{w} = \\mathbf{\\Phi}^T \\mathbf{y}. Usage First, instantiate the class RbfInterpolation . Via the constructor, an RBF can be specified from the following options: Gaussian ThinPlateSpline InverseQuadratic Linear By default, ThinPlateSpline (i.e., \\phi(x) = x^2 \\log(x) ) is chosen. Then, set the target scattered data by the method: void SetData(const Eigen::MatrixXd& X, const Eigen::VectorXd& y); where \\mathbf{X} = \\begin{bmatrix} \\mathbf{x}_{1} & \\cdots & \\mathbf{x}_{n} \\end{bmatrix} \\in \\mathbb{R}^{m \\times n} represents the data points and \\mathbf{y} = \\begin{bmatrix} y_1 & \\cdots & y_n \\end{bmatrix}^T represents their values. Next, calculate the weight values by the method: void ComputeWeights(bool use_regularization = false, double lambda = 0.001); When use_regularization is set true , the weights are calculated in the manner of scattered data approximation, rather than scattered data interpolation. When the data is noisy, approximation is usually a better choice. Once the above procedures are performed, the instance is ready to calculate interpolated values. This is performed by the method double GetValue(const Eigen::VectorXd& x) const; Useful Resources Ken Anjyo, J. P. Lewis, and Fr\u00e9d\u00e9ric Pighin. 2014. Scattered data interpolation for computer graphics. In ACM SIGGRAPH 2014 Courses (SIGGRAPH '14). ACM, New York, NY, USA, Article 27, 69 pages. DOI: https://doi.org/10.1145/2614028.2615425","title":"rbf-interpolation"},{"location":"rbf-interpolation/#rbf-interpolation","text":"Radial basis function (RBF) network for scattered data interpolation and function approximation.","title":"rbf-interpolation"},{"location":"rbf-interpolation/#header","text":"#include <mathtoolbox/rbf-interpolation.hpp>","title":"Header"},{"location":"rbf-interpolation/#math","text":"","title":"Math"},{"location":"rbf-interpolation/#overview","text":"Given input data: \\{ (\\mathbf{x}_i, y_i) \\}_{i = 1, \\ldots, n}, this technique calculates an interpolated value y for a specified point \\mathbf{x} by y = f(\\mathbf{x}) = \\sum_{i = 1}^{n} w_{i} \\phi( \\| \\mathbf{x} - \\mathbf{x}_{i} \\|), where \\phi(\\cdot) is a user-selected RBF and w_1, \\cdots, w_n are weights that are calculated in pre-computation.","title":"Overview"},{"location":"rbf-interpolation/#pre-computation","text":"The weight values need to be calculated in pre-computation. Let \\mathbf{w} = \\begin{bmatrix} w_1 & \\cdots & w_n \\end{bmatrix}^T and \\mathbf{\\Phi} = \\begin{bmatrix} \\phi_{1, 1} & \\cdots & \\phi_{1, n} \\\\ \\vdots & \\ddots & \\vdots \\\\ \\phi_{n, 1} & \\cdots & \\phi_{n, n} \\end{bmatrix}, where \\phi_{i, j} = \\phi(\\| \\mathbf{x}_i - \\mathbf{x}_j \\|). The following linear system is solved for \\mathbf{w} : \\mathbf{\\Phi} \\mathbf{w} = \\mathbf{y}. LU decomposition can be used for solving this problem.","title":"Pre-Computation"},{"location":"rbf-interpolation/#pre-computation-with-regularization","text":"The original formulation above is not robust (i.e., overfitting can occur) when the data points are dense and noisy. For such scenarios, it is possible to add a regularization term into pre-computation. That is, the following minimization problem is solved: \\min_{\\mathbf{w}} \\left\\{ \\| \\mathbf{\\Phi} \\mathbf{w} - \\mathbf{y} \\|^2 + \\lambda \\| \\mathbf{w} \\|^2 \\right\\}. The derivative of this objective function with respect to \\mathbf{w} is \\begin{eqnarray*} && \\frac{\\partial}{\\partial \\mathbf{w}} \\left\\{ \\| \\mathbf{\\Phi} \\mathbf{w} - \\mathbf{y} \\|^2 + \\lambda \\| \\mathbf{w} \\|^2 \\right\\} \\\\ &=& \\frac{\\partial}{\\partial \\mathbf{w}} \\| \\mathbf{\\Phi} \\mathbf{w} - \\mathbf{y} \\|^2 + \\lambda \\frac{\\partial}{\\partial \\mathbf{w}} \\| \\mathbf{w} \\|^2 \\\\ &=& 2 \\mathbf{\\Phi}^T (\\mathbf{\\Phi} \\mathbf{w} - \\mathbf{y}) + 2 \\lambda \\mathbf{w} \\\\ &=& 2 \\left\\{ (\\mathbf{\\Phi}^T \\mathbf{\\Phi} + \\lambda \\mathbf{I}) \\mathbf{w} - \\mathbf{\\Phi}^T \\mathbf{y} \\right\\}. \\end{eqnarray*} Thus, the solution of the above minimization problem can be obtained by solving the below linear system: (\\mathbf{\\Phi}^T \\mathbf{\\Phi} + \\lambda \\mathbf{I}) \\mathbf{w} = \\mathbf{\\Phi}^T \\mathbf{y}.","title":"Pre-Computation with Regularization"},{"location":"rbf-interpolation/#usage","text":"First, instantiate the class RbfInterpolation . Via the constructor, an RBF can be specified from the following options: Gaussian ThinPlateSpline InverseQuadratic Linear By default, ThinPlateSpline (i.e., \\phi(x) = x^2 \\log(x) ) is chosen. Then, set the target scattered data by the method: void SetData(const Eigen::MatrixXd& X, const Eigen::VectorXd& y); where \\mathbf{X} = \\begin{bmatrix} \\mathbf{x}_{1} & \\cdots & \\mathbf{x}_{n} \\end{bmatrix} \\in \\mathbb{R}^{m \\times n} represents the data points and \\mathbf{y} = \\begin{bmatrix} y_1 & \\cdots & y_n \\end{bmatrix}^T represents their values. Next, calculate the weight values by the method: void ComputeWeights(bool use_regularization = false, double lambda = 0.001); When use_regularization is set true , the weights are calculated in the manner of scattered data approximation, rather than scattered data interpolation. When the data is noisy, approximation is usually a better choice. Once the above procedures are performed, the instance is ready to calculate interpolated values. This is performed by the method double GetValue(const Eigen::VectorXd& x) const;","title":"Usage"},{"location":"rbf-interpolation/#useful-resources","text":"Ken Anjyo, J. P. Lewis, and Fr\u00e9d\u00e9ric Pighin. 2014. Scattered data interpolation for computer graphics. In ACM SIGGRAPH 2014 Courses (SIGGRAPH '14). ACM, New York, NY, USA, Article 27, 69 pages. DOI: https://doi.org/10.1145/2614028.2615425","title":"Useful Resources"},{"location":"strong-wolfe-conditions-line-search/","text":"strong-wolfe-conditions-line-search A line search method for finding a step size that satisfies the strong Wolfe conditions (i.e., the Armijo (i.e., sufficient decrease) condition and the curvature condition). Header #include <mathtoolbox/strong-wolfe-conditions-line-search.hpp> Math and Algorithm We follow Nocedal and Wright (2006) (Chapter 3, specifically Algorithm 3.5). Useful Resources Jorge Nocedal and Stephen J. Wright. 2006. Numerical optimization (2nd ed.). Springer. DOI: https://doi.org/10.1007/978-0-387-40065-5","title":"strong-wolfe-conditions-line-search"},{"location":"strong-wolfe-conditions-line-search/#strong-wolfe-conditions-line-search","text":"A line search method for finding a step size that satisfies the strong Wolfe conditions (i.e., the Armijo (i.e., sufficient decrease) condition and the curvature condition).","title":"strong-wolfe-conditions-line-search"},{"location":"strong-wolfe-conditions-line-search/#header","text":"#include <mathtoolbox/strong-wolfe-conditions-line-search.hpp>","title":"Header"},{"location":"strong-wolfe-conditions-line-search/#math-and-algorithm","text":"We follow Nocedal and Wright (2006) (Chapter 3, specifically Algorithm 3.5).","title":"Math and Algorithm"},{"location":"strong-wolfe-conditions-line-search/#useful-resources","text":"Jorge Nocedal and Stephen J. Wright. 2006. Numerical optimization (2nd ed.). Springer. DOI: https://doi.org/10.1007/978-0-387-40065-5","title":"Useful Resources"}]}